import os
import json
import pickle
import numpy as np
import torch
from torch.utils.data import Dataset


def collate_fn_bulk_372(batch):
    slide_ids = [item['slide_id'] for item in batch]
    patient_ids = [item['patient_id'] for item in batch]
    spot_graphs_list = [item['spot_graphs'] for item in batch]
    
    # ç¡®ä¿æ‰€æœ‰expressionå¼ é‡å¤§å°ä¸€è‡´
    expression_list = [item['expression'] for item in batch]
    if len(expression_list) > 0:
        # è·å–ç›®æ ‡å¤§å°ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªéç©ºå¼ é‡çš„å¤§å°ï¼‰
        target_size = expression_list[0].shape[0] if len(expression_list[0].shape) > 0 else expression_list[0].numel()
        # ç¡®ä¿æ‰€æœ‰å¼ é‡å¤§å°ä¸€è‡´
        normalized_expressions = []
        for expr in expression_list:
            if expr.shape[0] != target_size:
                # å¦‚æœå¤§å°ä¸ä¸€è‡´ï¼Œè¿›è¡Œå¡«å……æˆ–æˆªæ–­
                if expr.shape[0] < target_size:
                    # å¡«å……é›¶
                    padding = torch.zeros(target_size - expr.shape[0], dtype=expr.dtype, device=expr.device)
                    expr = torch.cat([expr, padding])
                else:
                    # æˆªæ–­
                    expr = expr[:target_size]
            normalized_expressions.append(expr)
        expressions = torch.stack(normalized_expressions)
    else:
        expressions = torch.empty((0,))
    
    all_cell_features_list = [item['all_cell_features'] for item in batch]
    all_cell_positions_list = [item['all_cell_positions'] for item in batch]
    cluster_labels_list = [item['cluster_labels'] for item in batch]
    has_graphs_list = [item['has_graphs'] for item in batch]
    cell_mappings_list = [item['cell_mapping'] for item in batch]

    return {
        'slide_ids': slide_ids,
        'patient_ids': patient_ids,
        'spot_graphs_list': spot_graphs_list,
        'expressions': expressions,
        'all_cell_features_list': all_cell_features_list,
        'all_cell_positions_list': all_cell_positions_list,
        'cluster_labels_list': cluster_labels_list,
        'has_graphs_list': has_graphs_list,
        'cell_mappings_list': cell_mappings_list
    }


class BulkStaticGraphDataset372(Dataset):
    def __init__(self, graph_data_dir, split='train', selected_genes=None, max_samples=None, fold_config=None, tpm_csv_file=None):
        super().__init__()
        self.graph_data_dir = graph_data_dir
        self.split = split
        self.selected_genes = selected_genes if selected_genes else []
        self.max_samples = max_samples
        self.fold_config = fold_config
        self.tpm_csv_file = tpm_csv_file
        self.load_graph_data()
        if self.fold_config:
            self.apply_fold_filter()
        print(f"åŠ è½½{split}é›†: {len(self.data_keys)}ä¸ªæ•°æ®é¡¹")
        if self.selected_genes:
            self.filter_genes()

    def load_graph_data(self):
        print(f"åŠ è½½{self.split}é›†çš„é™æ€å›¾æ•°æ®...")
        intra_graphs_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_intra_patch_graphs.pkl")
        inter_graphs_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_inter_patch_graphs.pkl")
        expressions_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_expressions.pkl")
        metadata_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_metadata.json")
        all_features_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_all_cell_features.pkl")
        all_positions_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_all_cell_positions.pkl")
        cluster_labels_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_cluster_labels.pkl")
        graph_status_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_graph_status.pkl")
        cell_mappings_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_cell_to_graph_mappings.pkl")
        slide_mappings_file = os.path.join(self.graph_data_dir, f"bulk_{self.split}_slide_to_patient_mapping.pkl")

        required_files = [intra_graphs_file, inter_graphs_file, expressions_file, metadata_file]
        for file_path in required_files:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Required file not found: {file_path}")

        with open(intra_graphs_file, 'rb') as f:
            self.intra_patch_graphs = pickle.load(f)
        with open(inter_graphs_file, 'rb') as f:
            self.inter_patch_graphs = pickle.load(f)

        print("åŠ è½½å®Œæ•´ç»†èƒç‰¹å¾æ•°æ®...")
        if os.path.exists(all_features_file):
            with open(all_features_file, 'rb') as f:
                self.all_cell_features = pickle.load(f)
            print(f"âœ… åŠ è½½äº†æ‰€æœ‰ç»†èƒçš„DINOç‰¹å¾æ•°æ®")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç»†èƒç‰¹å¾æ–‡ä»¶: {all_features_file}")
            self.all_cell_features = {}

        if os.path.exists(all_positions_file):
            with open(all_positions_file, 'rb') as f:
                self.all_cell_positions = pickle.load(f)
            print(f"âœ… åŠ è½½äº†æ‰€æœ‰ç»†èƒçš„ç©ºé—´åæ ‡æ•°æ®")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç©ºé—´åæ ‡æ–‡ä»¶: {all_positions_file}")
            self.all_cell_positions = {}

        if os.path.exists(cluster_labels_file):
            with open(cluster_labels_file, 'rb') as f:
                self.cluster_labels = pickle.load(f)
            print(f"âœ… åŠ è½½äº†æ‰€æœ‰ç»†èƒçš„èšç±»æ ‡ç­¾æ•°æ®")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°èšç±»æ ‡ç­¾æ–‡ä»¶: {cluster_labels_file}")
            self.cluster_labels = {}

        if os.path.exists(graph_status_file):
            with open(graph_status_file, 'rb') as f:
                self.graph_status = pickle.load(f)
            print(f"âœ… åŠ è½½äº†æ‚£è€…å›¾çŠ¶æ€æ•°æ®")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°å›¾çŠ¶æ€æ–‡ä»¶: {graph_status_file}")
            self.graph_status = {}

        if os.path.exists(cell_mappings_file):
            with open(cell_mappings_file, 'rb') as f:
                self.cell_to_graph_mappings = pickle.load(f)
            print(f"âœ… åŠ è½½äº†ç»†èƒåˆ°å›¾çš„æ˜ å°„æ•°æ®")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç»†èƒæ˜ å°„æ–‡ä»¶: {cell_mappings_file}")
            self.cell_to_graph_mappings = {}

        if os.path.exists(slide_mappings_file):
            with open(slide_mappings_file, 'rb') as f:
                self.slide_to_patient_mapping = pickle.load(f)
            print(f"âœ… åŠ è½½äº†åˆ‡ç‰‡åˆ°æ‚£è€…çš„æ˜ å°„æ•°æ®")
            self.slide_ids = list(self.intra_patch_graphs.keys())
            self.patient_ids = list(set(self.slide_to_patient_mapping.values()))
            print(f"  - åˆ‡ç‰‡æ•°: {len(self.slide_ids)}")
            print(f"  - æ¶‰åŠæ‚£è€…æ•°: {len(self.patient_ids)}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°åˆ‡ç‰‡æ˜ å°„æ–‡ä»¶ï¼Œå‡è®¾æ•°æ®æŒ‰æ‚£è€…ç»„ç»‡")
            self.slide_to_patient_mapping = {}
            self.slide_ids = []
            self.patient_ids = list(self.intra_patch_graphs.keys())

        print("ä½¿ç”¨ç­›é€‰åçš„897åŸºå› TPMæ•°æ®...")
        if self.tpm_csv_file is None:
            raise ValueError("tpm_csv_fileå‚æ•°æœªæä¾›ï¼Œè¯·åœ¨åˆ›å»ºdatasetæ—¶æŒ‡å®šTPMæ–‡ä»¶è·¯å¾„")
        import pandas as pd
        tpm_df = pd.read_csv(self.tpm_csv_file, index_col=0)
        
        # ğŸ”’ å…³é”®ä¿®å¤ï¼šåªåŠ è½½å½“å‰splitå¯¹åº”çš„æ‚£è€…TPMæ•°æ®ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²
        # å…ˆç¡®å®šå½“å‰splitåŒ…å«å“ªäº›æ‚£è€…ID
        current_split_patient_ids = set()
        if self.slide_to_patient_mapping:
            # æŒ‰åˆ‡ç‰‡ç»„ç»‡ï¼šä»slide_to_patient_mappingè·å–æ‚£è€…ID
            current_split_patient_ids = set(self.slide_to_patient_mapping.values())
        else:
            # æŒ‰æ‚£è€…ç»„ç»‡ï¼šç›´æ¥ä½¿ç”¨patient_ids
            current_split_patient_ids = set(self.patient_ids) if hasattr(self, 'patient_ids') else set()
        
        print(f"ğŸ”’ æ•°æ®æ³„éœ²é˜²æŠ¤ï¼šå½“å‰{self.split}é›†åŒ…å« {len(current_split_patient_ids)} ä¸ªæ‚£è€…")

        self.expressions_data = {}
        self.patient_id_mapping = {}
        for full_patient_id in tpm_df.columns:
            parts = full_patient_id.split('-')
            if len(parts) >= 4:
                truncated_id = '-'.join(parts[:4]) + '-01'
                # åªåŠ è½½å½“å‰splitçš„æ‚£è€…æ•°æ®
                if truncated_id in current_split_patient_ids:
                    self.expressions_data[truncated_id] = tpm_df[full_patient_id].values.astype(np.float32)
                    self.patient_id_mapping[truncated_id] = full_patient_id
            else:
                # åªåŠ è½½å½“å‰splitçš„æ‚£è€…æ•°æ®
                if full_patient_id in current_split_patient_ids:
                    self.expressions_data[full_patient_id] = tpm_df[full_patient_id].values.astype(np.float32)
                    self.patient_id_mapping[full_patient_id] = full_patient_id
        print(f"âœ… åŠ è½½äº† {len(self.expressions_data)} ä¸ªæ‚£è€…çš„897åŸºå› è¡¨è¾¾æ•°æ®ï¼ˆä»…å½“å‰{self.split}é›†ï¼‰")
        sample_patient = list(self.expressions_data.keys())[0]
        sample_sum = np.sum(self.expressions_data[sample_patient])
        print(f"éªŒè¯ - æ ·æœ¬æ‚£è€…è¡¨è¾¾å€¼æ€»å’Œ: {sample_sum:.2f}")

        with open(metadata_file, 'r') as f:
            self.metadata = json.load(f)

        if self.slide_to_patient_mapping:
            if self.max_samples is not None:
                self.slide_ids = self.slide_ids[:self.max_samples]
            self.data_keys = self.slide_ids
            print(f"âœ… æ•°æ®æŒ‰åˆ‡ç‰‡ç»„ç»‡: {len(self.slide_ids)} ä¸ªåˆ‡ç‰‡")
        else:
            self.patient_ids = list(self.expressions_data.keys())
            if self.max_samples is not None:
                self.patient_ids = self.patient_ids[:self.max_samples]
            self.data_keys = self.patient_ids
            print(f"âœ… æ•°æ®æŒ‰æ‚£è€…ç»„ç»‡: {len(self.patient_ids)} ä¸ªæ‚£è€…")

        items_with_graphs = 0
        items_without_graphs = 0
        for data_key in self.data_keys:
            has_graphs = self.graph_status.get(data_key, True)
            if has_graphs:
                items_with_graphs += 1
            else:
                items_without_graphs += 1
        print(f"æ•°æ®ç»Ÿè®¡:")
        if self.slide_to_patient_mapping:
            print(f"  - æ€»åˆ‡ç‰‡æ•°: {len(self.data_keys)}")
            print(f"  - æœ‰å›¾æ•°æ®åˆ‡ç‰‡: {items_with_graphs}")
            print(f"  - æ— å›¾æ•°æ®åˆ‡ç‰‡: {items_without_graphs} (ä»…ä½¿ç”¨åŸå§‹DINOç‰¹å¾)")
        else:
            print(f"  - æ€»æ‚£è€…æ•°: {len(self.data_keys)}")
            print(f"  - æœ‰å›¾æ•°æ®æ‚£è€…: {items_with_graphs}")
            print(f"  - æ— å›¾æ•°æ®æ‚£è€…: {items_without_graphs} (ä»…ä½¿ç”¨åŸå§‹DINOç‰¹å¾)")

        self.feature_dim = self.metadata.get('feature_dim', 128) if isinstance(self.metadata, dict) else 128
        self.original_num_genes = len(list(self.expressions_data.values())[0]) if self.expressions_data else 18080

    def apply_fold_filter(self):
        """æ ¹æ®fold_configè¿‡æ»¤æ•°æ®ï¼ˆå¦‚æœæä¾›äº†fold_configï¼‰"""
        if not self.fold_config:
            return
        # å¦‚æœæä¾›äº†fold_configï¼Œå¯ä»¥åœ¨è¿™é‡Œå®ç°äº¤å‰éªŒè¯çš„æ•°æ®è¿‡æ»¤é€»è¾‘
        # ç›®å‰æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºä¸»è¦çš„æ•°æ®åˆ’åˆ†å·²ç»é€šè¿‡splitå‚æ•°å®Œæˆ
        pass

    def filter_genes(self):
        if not self.selected_genes:
            return
        target_gene_count = len(self.selected_genes)
        filtered_expressions = {}
        for patient_id, expression_data in self.expressions_data.items():
            if isinstance(expression_data, np.ndarray):
                filtered_expressions[patient_id] = expression_data[:target_gene_count]
            else:
                filtered_expressions[patient_id] = np.zeros(target_gene_count)
        self.expressions_data = filtered_expressions
        self.num_genes = target_gene_count
        print(f"åŸºå› è¿‡æ»¤å®Œæˆï¼Œæœ€ç»ˆåŸºå› æ•°é‡: {self.num_genes}")
        if filtered_expressions:
            sample_patient = list(filtered_expressions.keys())[0]
            sample_data = filtered_expressions[sample_patient]
            sample_total = np.sum(sample_data)
            print(f"TPMæ•°æ®éªŒè¯ï¼šæ ·æœ¬æ‚£è€… {sample_patient} è¡¨è¾¾å€¼æ€»å’Œ: {sample_total:.2f}")

    def __len__(self):
        return len(self.data_keys)

    def __getitem__(self, idx):
        data_key = self.data_keys[idx]
        if self.slide_to_patient_mapping:
            slide_id = data_key
            patient_id = self.slide_to_patient_mapping[slide_id]
        else:
            slide_id = data_key
            patient_id = data_key
        intra_graphs = self.intra_patch_graphs.get(data_key, {})
        expression = self.expressions_data.get(patient_id, np.zeros(getattr(self, 'num_genes', self.original_num_genes)))
        all_cell_features = self.all_cell_features.get(data_key, torch.empty((0, self.feature_dim)))
        all_cell_positions = self.all_cell_positions.get(data_key, torch.empty((0, 2)))
        cluster_labels = self.cluster_labels.get(data_key, torch.empty((0,)))
        has_graphs = self.graph_status.get(data_key, False)
        cell_mapping = self.cell_to_graph_mappings.get(data_key, None)
        spot_graphs = list(intra_graphs.values())
        if isinstance(expression, np.ndarray):
            expression = torch.tensor(expression, dtype=torch.float32)
        else:
            expression = torch.tensor(np.zeros(getattr(self, 'num_genes', self.original_num_genes)), dtype=torch.float32)
        if not isinstance(all_cell_features, torch.Tensor):
            all_cell_features = torch.empty((0, self.feature_dim))
        if not isinstance(all_cell_positions, torch.Tensor):
            all_cell_positions = torch.empty((0, 2))
        if not isinstance(cluster_labels, torch.Tensor):
            cluster_labels = torch.empty((0,))
        return {
            'slide_id': slide_id,
            'patient_id': patient_id,
            'spot_graphs': spot_graphs,
            'expression': expression,
            'all_cell_features': all_cell_features,
            'all_cell_positions': all_cell_positions,
            'cluster_labels': cluster_labels,
            'has_graphs': has_graphs,
            'cell_mapping': cell_mapping
        }

