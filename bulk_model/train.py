#!/usr/bin/env python3
import os
import sys
import argparse
import psutil

# å…è®¸ä»é¡¹ç›®æ ¹ç›®å½•ç›´æ¥è¿è¡Œï¼šå°†é¡¹ç›®æ ¹åŠ å…¥ sys.path
_current_file_dir = os.path.dirname(os.path.abspath(__file__))
_project_root = os.path.dirname(_current_file_dir)
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)
import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import DataLoader

from bulk_model.utils import check_environment_compatibility, load_gene_mapping
from bulk_model.dataset import BulkStaticGraphDataset372, collate_fn_bulk_372
from bulk_model.models import OptimizedTransformerPredictor
from bulk_model.trainer import train_optimized_model, load_spatial_pretrained_weights

# TF32/AMP ä¼˜åŒ–
torch.set_float32_matmul_precision("medium")
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# ä¸åŸè„šæœ¬ä¸€è‡´ï¼šè¿è¡Œæ—¶åšä¸€æ¬¡ç¯å¢ƒæ£€æŸ¥
check_environment_compatibility()


def parse_args():
    parser = argparse.ArgumentParser(description='Bulk Model Training with LoRA and Optimizations')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument("--graph-data-dir", type=str, 
                       default="/root/autodl-tmp/bulk_BRCA_graphs_new_all_graph",
                       help="å›¾æ•°æ®ç›®å½•")
    parser.add_argument("--gene-list-file", type=str,
                       default="/root/autodl-tmp/her_hvg_cut_1000.txt",
                       help="åŸºå› åˆ—è¡¨æ–‡ä»¶")
    parser.add_argument("--features-file", type=str,
                       default="/root/autodl-tmp/features.tsv",
                       help="ç‰¹å¾æ–‡ä»¶")
    parser.add_argument("--tpm-csv-file", type=str,
                       default="/root/autodl-tmp/tpm-TCGA-BRCA-1000-million.csv",
                       help="TPMè¡¨è¾¾æ•°æ®CSVæ–‡ä»¶")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch-size", type=int, default=4,
                       help="æ‚£è€…Batch Size (None=åŠ¨æ€æœç´¢)")
    parser.add_argument("--graph-batch-size", type=int, default=128,
                       help="å›¾Batch Size")
    parser.add_argument("--num-epochs", type=int, default=30,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning-rate", type=float, default=1e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--weight-decay", type=float, default=1e-5,
                       help="æƒé‡è¡°å‡")
    parser.add_argument("--patience", type=int, default=15,
                       help="æ—©åœè€å¿ƒå€¼")

    # è¿ç§»å­¦ä¹ ï¼šä» spatial æ¨¡å‹åˆå§‹åŒ–
    parser.add_argument("--spatial-model-path", type=str, default=None,
                       help="å¯é€‰ï¼šä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„ spatial æ¨¡å‹ checkpoint ä½œä¸º bulk é¢„è®­ç»ƒæƒé‡")
    parser.add_argument("--freeze-backbone-from-spatial", action="store_true",
                       help="å¦‚æœæä¾› spatial æ¨¡å‹ï¼Œæ˜¯å¦å†»ç»“ bulk çš„ GNN + feature_projection + transformerï¼Œä»…è®­ç»ƒè¾“å‡ºå¤´")
    
    # LoRA å‚æ•°
    parser.add_argument("--use-lora", action="store_true", default=True,
                       help="ä½¿ç”¨LoRA")
    parser.add_argument("--lora-r", type=int, default=8,
                       help="LoRA rank")
    parser.add_argument("--lora-alpha", type=int, default=16,
                       help="LoRA alpha")
    parser.add_argument("--lora-dropout", type=float, default=0.05,
                       help="LoRA dropout")
    parser.add_argument("--lora-freeze-base", action="store_true", default=True,
                       help="å†»ç»“LoRAåŸºç¡€æƒé‡")
    
    # æ•°æ®åŠ è½½å™¨ä¼˜åŒ–
    parser.add_argument("--num-workers-train", type=int, default=0,
                       help="è®­ç»ƒæ•°æ®åŠ è½½å™¨workeræ•° (None=è‡ªåŠ¨)")
    parser.add_argument("--num-workers-test", type=int, default=0,
                       help="æµ‹è¯•æ•°æ®åŠ è½½å™¨workeræ•° (None=è‡ªåŠ¨)")
    parser.add_argument("--pin-memory", action="store_true", default=True,
                       help="å¯ç”¨pin_memory")
    parser.add_argument("--persistent-workers", action="store_true", default=True,
                       help="å¯ç”¨persistent_workers")
    parser.add_argument("--prefetch-factor", type=int, default=1,
                       help="é¢„å–å› å­")
    
    # è®­ç»ƒä¼˜åŒ–
    parser.add_argument("--log-every", type=int, default=10,
                       help="debugæ—¥å¿—çš„batché—´éš”")
    parser.add_argument("--debug-logs", action="store_true",
                       help="å¼€å¯è¯¦ç»†è°ƒè¯•æ—¥å¿—")
    parser.add_argument("--enable-profiling", action="store_true",
                       help="å¼€å¯batchçº§æ€§èƒ½ç›‘æ§")
    parser.add_argument("--cleanup-interval", type=int, default=1,
                       help="æ˜¾å­˜æ¸…ç†é—´éš” (batchæ•°)")
    parser.add_argument("--max-train-samples", type=int, default=None,
                       help="æœ€å¤§è®­ç»ƒæ ·æœ¬æ•° (None=å…¨éƒ¨)")
    
    # åŠ¨æ€batch size
    parser.add_argument("--disable-dynamic-bsz", action="store_true",
                       help="ç¦ç”¨åŠ¨æ€batch sizeæœç´¢")
    parser.add_argument("--max-dynamic-bsz", type=int, default=8,
                       help="åŠ¨æ€batch sizeæœç´¢æœ€å¤§å€¼")
    # Gene normalization control (default: enabled)
    group_norm = parser.add_mutually_exclusive_group()
    group_norm.add_argument("--apply-gene-normalization", dest="apply_gene_normalization", action="store_true",
                            help="å¯ç”¨åŸºå›  z-score å½’ä¸€åŒ– (é»˜è®¤)")
    group_norm.add_argument("--no-gene-normalization", dest="apply_gene_normalization", action="store_false",
                            help="ç¦ç”¨åŸºå›  z-score å½’ä¸€åŒ–")
    parser.set_defaults(apply_gene_normalization=True)

    # Use gene-attention readout for bulk (default: False; keep original per-node path)
    parser.add_argument("--use-gene-attention-bulk", action="store_true", default=True,
                        help="åœ¨ bulk æ¨¡å‹ä¸­ä½¿ç”¨ gene-level attention ç›´æ¥è¾“å‡ºåŸºå› é¢„æµ‹ï¼ˆæŒ‰ WSI/æ‚£è€… èšåˆï¼›é»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--cluster-loss-weight", type=float, default=0.1,
                       help="èšç±»æŸå¤±æƒé‡ (cluster loss weight). é»˜è®¤ 0.1")
    
    return parser.parse_args()


def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def format_number(num):
    """æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º"""
    if num >= 1e9:
        return f"{num / 1e9:.2f}B"
    elif num >= 1e6:
        return f"{num / 1e6:.2f}M"
    elif num >= 1e3:
        return f"{num / 1e3:.2f}K"
    else:
        return str(num)


def find_optimal_batch_size(dataset, device, max_batch_size=16):
    """åŠ¨æ€å¯»æ‰¾æœ€ä½³batch size"""
    print("ğŸ” æ­£åœ¨å¯»æ‰¾æœ€ä½³batch_size...")
    
    for bs in [2, 4, 8, 16]:
        if bs > max_batch_size:
            break
        
        try:
            print(f"  æµ‹è¯• batch_size={bs}")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
            test_loader = DataLoader(
                dataset,
                batch_size=bs,
                num_workers=2,
                pin_memory=True
            )
            
            # æµ‹è¯•ä¸€ä¸ªbatch
            for batch in test_loader:
                expressions = batch['expressions'].to(device)
                spot_graphs_list = batch['spot_graphs_list']
                
                # ä¼°ç®—æ˜¾å­˜ä½¿ç”¨
                torch.cuda.empty_cache()
                before_mem = torch.cuda.memory_allocated(device)
                
                # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
                dummy_prediction = torch.randn(
                    expressions.shape,
                    device=device,
                    requires_grad=True
                )
                loss = nn.MSELoss()(dummy_prediction, expressions)
                loss.backward()
                
                after_mem = torch.cuda.memory_allocated(device)
                mem_usage_gb = (after_mem - before_mem) / 1024**3
                
                print(f"    âœ… batch_size={bs} å¯è¡Œï¼Œæ˜¾å­˜ä½¿ç”¨â‰ˆ{mem_usage_gb:.1f}GB")
                
                del dummy_prediction, loss, expressions, spot_graphs_list
                torch.cuda.empty_cache()
                break
        
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"    âŒ batch_size={bs} æ˜¾å­˜ä¸è¶³")
                torch.cuda.empty_cache()
                return max(1, bs // 2)
            else:
                print(f"    âŒ batch_size={bs} å…¶ä»–é”™è¯¯: {e}")
                torch.cuda.empty_cache()
                continue
    
    return min(bs, max_batch_size)


def main():
    args = parse_args()
    print("=== ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡å¤„ç†å¤šå›¾æå‡GPUåˆ©ç”¨ç‡ + LoRA ===")

    selected_genes, _ = load_gene_mapping(args.gene_list_file, args.features_file)
    if not selected_genes:
        print("é”™è¯¯: æœªèƒ½åŠ è½½åŸºå› æ˜ å°„")
        return
    print(f"æœ€ç»ˆåŸºå› æ•°é‡: {len(selected_genes)}")

    train_dataset = BulkStaticGraphDataset372(
        graph_data_dir=args.graph_data_dir,
        split='train',
        selected_genes=selected_genes,
        max_samples=args.max_train_samples,
        tpm_csv_file=args.tpm_cfile if False else args.tpm_csv_file,
        apply_gene_normalization=args.apply_gene_normalization
    )
    # If gene normalization is enabled in the training dataset, propagate stats to the test dataset
    normalization_stats = None
    if getattr(train_dataset, "apply_gene_normalization", False):
        normalization_stats = getattr(train_dataset, "normalization_stats", None)
    test_dataset = BulkStaticGraphDataset372(
        graph_data_dir=args.graph_data_dir,
        split='test',
        selected_genes=selected_genes,
        max_samples=None,
        tpm_csv_file=args.tpm_csv_file,
        apply_gene_normalization=getattr(train_dataset, "apply_gene_normalization", False),
        normalization_stats=normalization_stats
    )
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, æµ‹è¯•æ ·æœ¬: {len(test_dataset)}")

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # åŠ¨æ€æˆ–æ‰‹åŠ¨é…ç½®batch_size
    initial_batch_size = 2
    if args.batch_size is not None:
        batch_size = max(1, args.batch_size)
        print(f"ä½¿ç”¨æŒ‡å®šçš„æ‚£è€…Batch Size: {batch_size}")
    elif args.disable_dynamic_bsz:
        batch_size = initial_batch_size
        print(f"ç¦ç”¨åŠ¨æ€batch sizeï¼Œä½¿ç”¨åˆå§‹å€¼: {batch_size}")
    else:
        optimal_batch_size = find_optimal_batch_size(train_dataset, device, max_batch_size=args.max_dynamic_bsz)
        batch_size = max(initial_batch_size, optimal_batch_size)
        print(f"åŠ¨æ€æœç´¢å¾—åˆ°æ‚£è€…Batch Size: {batch_size}")

    graph_batch_size = args.graph_batch_size
    print(f"å›¾Batch Size: {graph_batch_size} (å¢å¼ºGPUåˆ©ç”¨ç‡)")

    # æ•°æ®åŠ è½½å™¨ä¼˜åŒ–é…ç½®
    cpu_cores = psutil.cpu_count(logical=False) or 1
    logical_cores = psutil.cpu_count(logical=True) or cpu_cores
    print(f"CPUæ ¸å¿ƒï¼šç‰©ç†={cpu_cores}, é€»è¾‘={logical_cores}")
    
    num_workers_train = args.num_workers_train if args.num_workers_train is not None else min(8, cpu_cores)
    num_workers_test = args.num_workers_test if args.num_workers_test is not None else min(4, cpu_cores)

    def build_loader(dataset, shuffle, num_workers):
        kwargs = dict(
            batch_size=batch_size,
            shuffle=shuffle,
            collate_fn=collate_fn_bulk_372,
            num_workers=max(0, num_workers),
            pin_memory=args.pin_memory,
            drop_last=False,
        )
        if num_workers > 0:
            kwargs["persistent_workers"] = args.persistent_workers
            if args.prefetch_factor is not None:
                kwargs["prefetch_factor"] = max(1, args.prefetch_factor)
        else:
            kwargs["pin_memory"] = False
        return DataLoader(dataset, **kwargs)

    train_loader = build_loader(train_dataset, shuffle=True, num_workers=num_workers_train)
    test_loader = build_loader(test_dataset, shuffle=False, num_workers=num_workers_test)

    print(f"æ•°æ®åŠ è½½å™¨ï¼štrain_workers={num_workers_train}, test_workers={num_workers_test}, pin_memory={args.pin_memory}")

    # è®¡ç®—ä½¿ç”¨LoRAå‰åçš„å‚æ•°é‡å¯¹æ¯”
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹å‚æ•°é‡ç»Ÿè®¡")
    print("="*60)
    
    if args.use_lora:
        print("æ­£åœ¨å¯¹æ¯”LoRAå‰åçš„å‚æ•°é‡...")
        
        # åˆ›å»ºä¸å¸¦LoRAçš„ä¸´æ—¶æ¨¡å‹æ¥è®¡ç®—åŸå§‹å‚æ•°é‡
        print("  1. åˆ›å»ºä¸å¸¦LoRAçš„æ¨¡å‹ä»¥è®¡ç®—åŸå§‹å‚æ•°é‡...")
        model_without_lora = OptimizedTransformerPredictor(
            input_dim=train_dataset.feature_dim,
            gnn_hidden_dim=128,
            gnn_output_dim=128,
            embed_dim=256,
            num_genes=train_dataset.num_genes,
            num_layers=3,
            nhead=8,
            dropout=0.1,
            use_gnn=True,
            gnn_type='GAT',
            graph_batch_size=graph_batch_size,
            use_lora=False,  # ä¸ä½¿ç”¨LoRA
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            lora_freeze_base=args.lora_freeze_base
        )
        
        total_params_wo_lora, trainable_params_wo_lora = count_parameters(model_without_lora)
        print(f"     âœ“ ä¸å¸¦LoRAæ¨¡å‹å‚æ•°é‡è®¡ç®—å®Œæˆ")
        
        # åˆ›å»ºå¸¦LoRAçš„å®é™…æ¨¡å‹ï¼ˆåç»­å¯é€‰åœ°ä» spatial åˆå§‹åŒ–ï¼‰
        print("  2. åˆ›å»ºå¸¦LoRAçš„æ¨¡å‹...")
        model = OptimizedTransformerPredictor(
            input_dim=train_dataset.feature_dim,
            gnn_hidden_dim=128,
            gnn_output_dim=128,
            embed_dim=256,
            num_genes=train_dataset.num_genes,
            num_layers=3,
            nhead=8,
            dropout=0.1,
            use_gnn=True,
            gnn_type='GAT',
            graph_batch_size=graph_batch_size,
            use_lora=args.use_lora,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            lora_freeze_base=args.lora_freeze_base
        )

        # å¦‚æœç”¨æˆ·æä¾›äº† spatial checkpointï¼Œåˆ™åœ¨æ­¤åŸºç¡€ä¸Šåšè¿ç§»å­¦ä¹ åˆå§‹åŒ–
        if args.spatial_model_path:
            model = load_spatial_pretrained_weights(
                model,
                spatial_checkpoint_path=args.spatial_model_path,
                device=device,
                freeze_backbone=args.freeze_backbone_from_spatial,
            )
        
        total_params_with_lora, trainable_params_with_lora = count_parameters(model)
        print(f"     âœ“ å¸¦LoRAæ¨¡å‹å‚æ•°é‡è®¡ç®—å®Œæˆ")
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        print("\n" + "-"*60)
        print("å‚æ•°é‡å¯¹æ¯”ç»“æœï¼ˆLoRAå‰åï¼‰ï¼š")
        print("-"*60)
        print(f"{'æŒ‡æ ‡':<20} {'ä¸å¸¦LoRA':<20} {'å¸¦LoRA':<20} {'å˜åŒ–':<20}")
        print("-"*60)
        
        # æ€»å‚æ•°é‡
        total_diff = total_params_with_lora - total_params_wo_lora
        total_diff_pct = (total_diff / total_params_wo_lora * 100) if total_params_wo_lora > 0 else 0
        print(f"{'æ€»å‚æ•°é‡':<20} {format_number(total_params_wo_lora):<20} {format_number(total_params_with_lora):<20} "
              f"{total_diff_pct:+.2f}% ({format_number(total_diff)})")
        
        # å¯è®­ç»ƒå‚æ•°é‡
        trainable_diff = trainable_params_with_lora - trainable_params_wo_lora
        trainable_diff_pct = (trainable_diff / trainable_params_wo_lora * 100) if trainable_params_wo_lora > 0 else 0
        reduction_pct = ((trainable_params_wo_lora - trainable_params_with_lora) / trainable_params_wo_lora * 100) if trainable_params_wo_lora > 0 else 0
        
        print(f"{'å¯è®­ç»ƒå‚æ•°é‡':<20} {format_number(trainable_params_wo_lora):<20} {format_number(trainable_params_with_lora):<20} "
              f"{trainable_diff_pct:+.2f}% ({format_number(trainable_diff)})")
        
        if trainable_params_with_lora < trainable_params_wo_lora:
            print(f"\nğŸ‰ LoRAå‡å°‘äº† {reduction_pct:.2f}% çš„å¯è®­ç»ƒå‚æ•°ï¼")
            print(f"   å¯è®­ç»ƒå‚æ•°ä» {format_number(trainable_params_wo_lora)} å‡å°‘åˆ° {format_number(trainable_params_with_lora)}")
            print(f"   å‡å°‘äº† {format_number(trainable_params_wo_lora - trainable_params_with_lora)} ä¸ªå¯è®­ç»ƒå‚æ•°")
        
        # å†»ç»“å‚æ•°æ•°é‡
        frozen_params_wo_lora = total_params_wo_lora - trainable_params_wo_lora
        frozen_params_with_lora = total_params_with_lora - trainable_params_with_lora
        print(f"\n{'å†»ç»“å‚æ•°é‡':<20} {format_number(frozen_params_wo_lora):<20} {format_number(frozen_params_with_lora):<20} "
              f"{format_number(frozen_params_with_lora - frozen_params_wo_lora)}")
        
        print("="*60 + "\n")
        
        # æ¸…ç†ä¸´æ—¶æ¨¡å‹
        del model_without_lora
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    else:
        # æœªå¯ç”¨LoRAï¼Œåªæ˜¾ç¤ºå½“å‰æ¨¡å‹çš„å‚æ•°é‡
        print("LoRAæœªå¯ç”¨ï¼Œä»…æ˜¾ç¤ºå½“å‰æ¨¡å‹å‚æ•°é‡...")
        model = OptimizedTransformerPredictor(
            input_dim=train_dataset.feature_dim,
            gnn_hidden_dim=128,
            gnn_output_dim=128,
            embed_dim=256,
            num_genes=train_dataset.num_genes,
            num_layers=3,
            nhead=8,
            dropout=0.1,
            use_gnn=True,
            gnn_type='GAT',
            graph_batch_size=graph_batch_size,
            use_lora=False,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            lora_freeze_base=args.lora_freeze_base
        )

        if args.spatial_model_path:
            model = load_spatial_pretrained_weights(
                model,
                spatial_checkpoint_path=args.spatial_model_path,
                device=device,
                freeze_backbone=args.freeze_backbone_from_spatial,
            )
        
        total_params, trainable_params = count_parameters(model)
        frozen_params = total_params - trainable_params
        
        print("\n" + "-"*60)
        print("æ¨¡å‹å‚æ•°é‡ï¼š")
        print("-"*60)
        print(f"æ€»å‚æ•°é‡:     {format_number(total_params)}")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {format_number(trainable_params)}")
        print(f"å†»ç»“å‚æ•°é‡:   {format_number(frozen_params)}")
        print("="*60 + "\n")

    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epochs, eta_min=1e-7)

    # Configure model behavior: whether to use gene-attention readout for bulk
    if getattr(args, "use_gene_attention_bulk", False):
        print("âš™ï¸ é…ç½®ï¼šbulk ä½¿ç”¨ gene-attention readout è¾“å‡ºï¼ˆæŒ‰ WSI/æ‚£è€… èšåˆï¼‰")
        model.return_gene_level = True
    else:
        model.return_gene_level = False

    print(f"\n=== è®­ç»ƒé…ç½®ï¼ˆLoRA + ä¼˜åŒ–ç‰ˆæœ¬ï¼‰===")
    print(f"å›¾æ‰¹é‡å¤„ç†å¤§å°: {graph_batch_size}")
    print(f"LoRA: r={args.lora_r}, alpha={args.lora_alpha}, dropout={args.lora_dropout}, freeze_base={args.lora_freeze_base}")
    print(f"æ”¯æŒæ··åˆå¤„ç†: æœ‰å›¾å¢å¼º + æ— å›¾åŸå§‹ç‰¹å¾")
    print(f"æ•°æ®ä¿ç•™ç‡: 100% (0%ä¸¢å¤±)")
    print(f"æ—¥å¿—ï¼šlog_every={args.log_every}, debug={args.debug_logs}, profiling={args.enable_profiling}")

    train_losses, test_losses = train_optimized_model(
        model, train_loader, test_loader, optimizer, scheduler,
        num_epochs=args.num_epochs, device=device, patience=args.patience,
        log_every=args.log_every, debug=args.debug_logs,
        enable_profiling=args.enable_profiling,
        cleanup_interval=args.cleanup_interval,
        cluster_loss_weight=args.cluster_loss_weight
    )

    print("\n=== æ··åˆå¤„ç†è®­ç»ƒå®Œæˆ! ===")
    print("âœ“ æ”¯æŒæœ‰å›¾æ‚£è€…ï¼ˆå›¾å¢å¼ºï¼‰å’Œæ— å›¾æ‚£è€…ï¼ˆåŸå§‹DINOç‰¹å¾ï¼‰")
    print("âœ“ LoRAå·²åº”ç”¨ï¼Œå‡å°‘å¯è®­ç»ƒå‚æ•°")
    print("âœ“ æ•°æ®ä¿ç•™ç‡: 100%ï¼Œ0%ä¸¢å¤±")
    print("âœ“ ä¿æŒåŸæœ‰è®¡ç®—é€»è¾‘ä¸å˜")


if __name__ == "__main__":
    main()


