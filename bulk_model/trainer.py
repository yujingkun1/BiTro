import os
import torch
import torch.nn as nn
from torch.amp import autocast, GradScaler
import matplotlib.pyplot as plt
import time
import numpy as np
import psutil
import gc


def load_spatial_pretrained_weights(
    model: nn.Module,
    spatial_checkpoint_path: str,
    device: str | torch.device = "cuda",
    freeze_backbone: bool = False,
) -> nn.Module:
    """
    Load compatible weights from a pretrained spatial model checkpoint
    into the bulk OptimizedTransformerPredictor to enable transfer learning.

    The spatial model is `spitial_model.models.StaticGraphTransformerPredictor`
    trained via `spitial_model/train_transfer_learning.py`.

    This function:
    - Loads the spatial checkpoint (state_dict)
    - Copies over matching keys with identical tensor shapes:
        * gnn.*
        * feature_projection.*
        * transformer.*
        * output_projection.*  (only when shape matches num_genes)
    - Skips spatial-specific parameters such as:
        * gene_queries, gene_readout
        * x_embed, y_embed
    - Optionally freezes backbone layers (GNN + feature_projection + transformer)
      while keeping output head trainable.
    """
    device = torch.device(device)

    if not spatial_checkpoint_path:
        print("âš  æœªæä¾› spatial æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡è¿ç§»å­¦ä¹ åˆå§‹åŒ–ã€‚")
        return model

    if not os.path.exists(spatial_checkpoint_path):
        print(f"âš  Spatial checkpoint æœªæ‰¾åˆ°: {spatial_checkpoint_path}")
        return model

    print("\n=== ä» Spatial é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ– Bulk æ¨¡å‹æƒé‡ ===")
    print(f"Spatial checkpoint: {spatial_checkpoint_path}")

    try:
        spatial_state = torch.load(spatial_checkpoint_path, map_location=device)
        # å…¼å®¹ç›´æ¥ä¿å­˜ state_dict æˆ–åŒ…å« 'state_dict' çš„æƒ…å†µ
        if isinstance(spatial_state, dict) and "state_dict" in spatial_state:
            spatial_state = spatial_state["state_dict"]
        if not isinstance(spatial_state, dict):
            raise ValueError("Spatial checkpoint ä¸åŒ…å«æœ‰æ•ˆçš„ state_dict å­—å…¸")

        model_state = model.state_dict()

        loaded_keys: list[str] = []
        skipped_keys: list[str] = []
        mismatched_keys: list[tuple[str, torch.Size, torch.Size]] = []

        for key, value in spatial_state.items():
            # è·³è¿‡æ˜æ˜¾çš„ç©ºé—´ç‰¹å¼‚å‚æ•°
            if any(
                s in key
                for s in [
                    "gene_queries",
                    "gene_readout",
                    "x_embed",
                    "y_embed",
                ]
            ):
                skipped_keys.append(key)
                continue

            if key in model_state:
                if model_state[key].shape == value.shape:
                    model_state[key] = value
                    loaded_keys.append(key)
                else:
                    mismatched_keys.append((key, model_state[key].shape, value.shape))
            else:
                # å…¶ä»–åœ¨ bulk æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„ key ä¹Ÿè·³è¿‡
                skipped_keys.append(key)

        model.load_state_dict(model_state, strict=False)

        print(f"âœ“ æˆåŠŸä» spatial æ¨¡å‹åŠ è½½ {len(loaded_keys)} å±‚å‚æ•°åˆ° bulk æ¨¡å‹")
        if loaded_keys:
            print(f"  ç¤ºä¾‹: {', '.join(loaded_keys[:5])}")

        if mismatched_keys:
            print(f"âš  å› å½¢çŠ¶ä¸åŒ¹é…è·³è¿‡ {len(mismatched_keys)} å±‚ï¼ˆä¾‹å¦‚è¾“å‡º head åŸºå› æ•°ä¸åŒï¼‰:")
            for k, ms, ss in mismatched_keys[:5]:
                print(f"  {k}: bulk {ms} vs spatial {ss}")
            if len(mismatched_keys) > 5:
                print(f"  ... ä»¥åŠå¦å¤– {len(mismatched_keys) - 5} å±‚")

        if skipped_keys:
            print(f"â„¹ è·³è¿‡ {len(skipped_keys)} ä¸ª spatial ç‰¹æœ‰æˆ– bulk ä¸­ä¸å­˜åœ¨çš„å‚æ•°ï¼ˆå¦‚ gene_queries/x_embed ç­‰ï¼‰")

        if freeze_backbone:
            print("\n=== å†»ç»“ Bulk æ¨¡å‹ Backboneï¼ˆGNN + feature_projection + transformerï¼‰===")
            frozen_params = 0
            trainable_params = 0

            # å†»ç»“ GNN
            if hasattr(model, "gnn"):
                for _, p in model.gnn.named_parameters():
                    p.requires_grad = False
                    frozen_params += p.numel()

            # å†»ç»“ç‰¹å¾æŠ•å½±
            if hasattr(model, "feature_projection"):
                for _, p in model.feature_projection.named_parameters():
                    p.requires_grad = False
                    frozen_params += p.numel()

            # å†»ç»“ transformer
            if hasattr(model, "transformer"):
                for _, p in model.transformer.named_parameters():
                    p.requires_grad = False
                    frozen_params += p.numel()

            # ç»Ÿè®¡ä»ç„¶å¯è®­ç»ƒçš„å‚æ•°ï¼ˆä¾‹å¦‚ output_projection çš„æœ€åå‡ å±‚ï¼‰
            for name, p in model.named_parameters():
                if p.requires_grad:
                    trainable_params += p.numel()

            print(f"âœ“ Backbone å†»ç»“å‚æ•°é‡: {frozen_params:,}")
            print(f"âœ“ ä»å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")

    except Exception as e:  # pylint: disable=broad-except
        print(f"âŒ ä» spatial checkpoint åŠ è½½æƒé‡å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ bulk æ¨¡å‹ç»§ç»­è®­ç»ƒã€‚")

    return model


def train_optimized_model(model, train_loader, test_loader, optimizer, scheduler=None,
                         num_epochs=50, device="cuda", patience=10, min_delta=1e-6,
                         log_every=10, debug=False, enable_profiling=False, cleanup_interval=1):
    model.to(device)
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda')

    best_loss = float('inf')
    best_test_loss = float('inf')
    early_stopping_counter = 0
    best_epoch = 0

    train_losses = []
    test_losses = []

    print("=== å¼€å§‹ä¼˜åŒ–è®­ç»ƒï¼ˆæ‰¹é‡å¤„ç†å¤šå›¾ï¼‰===")
    print(f"å›¾æ‰¹é‡å¤§å°: {model.graph_batch_size}")

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        num_batches = 0
        batch_skip_count = 0
        patient_skip_count = 0

        print(f"\n=== Epoch {epoch+1} å¼€å§‹è®­ç»ƒ ===")

        for batch_idx, batch in enumerate(train_loader):
            batch_start_time = time.time() if enable_profiling else None
            data_loading_time = 0.0
            
            expressions = batch['expressions'].to(device, non_blocking=True)
            spot_graphs_list = batch['spot_graphs_list']

            log_this_batch = (batch_idx % log_every == 0) or debug
            if log_this_batch:
                print(f"\nBatch {batch_idx}: å¼€å§‹å¤„ç† {len(spot_graphs_list)} ä¸ªæ‚£è€…")

            optimizer.zero_grad()

            batch_predictions = []

            for i in range(len(spot_graphs_list)):
                spot_graphs = batch['spot_graphs_list'][i]
                all_cell_features = batch['all_cell_features_list'][i]
                all_cell_positions = batch['all_cell_positions_list'][i]
                has_graphs = batch['has_graphs_list'][i]

                if log_this_batch:
                    print(f"  æ‚£è€… {i+1}: ç»†èƒç‰¹å¾å½¢çŠ¶={all_cell_features.shape}, ä½ç½®å½¢çŠ¶={all_cell_positions.shape}, æœ‰å›¾={has_graphs}, å›¾æ•°é‡={len(spot_graphs) if spot_graphs else 0}")

                if all_cell_features.shape[0] == 0:
                    if log_this_batch:
                        print(f"    âš ï¸ è·³è¿‡æ‚£è€… {i+1}ï¼šæ²¡æœ‰ç»†èƒç‰¹å¾æ•°æ®")
                    patient_skip_count += 1
                    continue

                all_cell_features = all_cell_features.to(device, non_blocking=True)
                all_cell_positions = all_cell_positions.to(device, non_blocking=True)

                if has_graphs and len(spot_graphs) > 0:
                    for graph in spot_graphs:
                        if hasattr(graph, 'x') and graph.x is not None:
                            graph.x = graph.x.to(device, non_blocking=True)
                        if hasattr(graph, 'edge_index') and graph.edge_index is not None:
                            graph.edge_index = graph.edge_index.to(device, non_blocking=True)

                forward_start_time = time.time() if enable_profiling else None

                with autocast('cuda'):
                    if has_graphs and len(spot_graphs) > 0:
                        total_cells = sum([graph.x.shape[0] for graph in spot_graphs if hasattr(graph, 'x') and graph.x is not None])
                        max_cells_threshold = 150000

                        if total_cells <= max_cells_threshold:
                            if log_this_batch:
                                print(f"    æœ‰å›¾å¤„ç†ï¼š{len(spot_graphs)}ä¸ªå›¾ â†’ {total_cells}ä¸ªç»†èƒ (å›¾å¢å¼º)")
                            cell_predictions_list = model(spot_graphs)
                        else:
                            if log_this_batch:
                                print(f"    è¶…å¤§æœ‰å›¾æ‚£è€…ï¼š{len(spot_graphs)}ä¸ªå›¾ â†’ {total_cells}ä¸ªç»†èƒ (æ¢¯åº¦ç´¯ç§¯åˆ†æ‰¹)")
                            target_cells_per_batch = 10000
                            batch_size_adaptive = max(32, len(spot_graphs) * target_cells_per_batch // total_cells)
                            all_cell_predictions_list = []
                            for batch_start in range(0, len(spot_graphs), batch_size_adaptive):
                                batch_end = min(batch_start + batch_size_adaptive, len(spot_graphs))
                                batch_graphs = spot_graphs[batch_start:batch_end]
                                batch_cells = sum([g.x.shape[0] for g in batch_graphs if hasattr(g, 'x')])
                                if log_this_batch:
                                    print(f"      åˆ†æ‰¹{batch_start//batch_size_adaptive + 1}: {len(batch_graphs)}ä¸ªå›¾ â†’ {batch_cells}ä¸ªç»†èƒ")
                                current_batch_predictions = model(batch_graphs)
                                all_cell_predictions_list.extend(current_batch_predictions)
                                torch.cuda.empty_cache()
                                del current_batch_predictions
                            cell_predictions_list = all_cell_predictions_list
                    else:
                        if log_this_batch:
                            print(f"    æ— å›¾å¤„ç†ï¼š{all_cell_features.shape[0]}ä¸ªç»†èƒ (åŸå§‹DINOç‰¹å¾)")
                        cell_predictions = model.forward_raw_features(all_cell_features, all_cell_positions)
                        cell_predictions_list = [cell_predictions]

                    if cell_predictions_list:
                        all_cell_predictions = torch.cat([pred for pred in cell_predictions_list if pred.shape[0] > 0], dim=0)
                        if all_cell_predictions.shape[0] > 0:
                            aggregated_prediction = all_cell_predictions.sum(dim=0, keepdim=True)
                            if log_this_batch:
                                print(f"    æ‚£è€… {i+1} é¢„æµ‹èšåˆï¼šç»†èƒæ•°={all_cell_predictions.shape[0]}, èšåˆç»“æœå½¢çŠ¶={aggregated_prediction.shape}")
                        else:
                            aggregated_prediction = torch.zeros(1, expressions.shape[1], device=device)
                            if log_this_batch:
                                print(f"    æ‚£è€… {i+1} é¢„æµ‹èšåˆï¼šæ²¡æœ‰æœ‰æ•ˆç»†èƒï¼Œä½¿ç”¨é›¶é¢„æµ‹")
                    else:
                        aggregated_prediction = torch.zeros(1, expressions.shape[1], device=device)
                        if log_this_batch:
                            print(f"    æ‚£è€… {i+1} é¢„æµ‹èšåˆï¼šæ²¡æœ‰é¢„æµ‹ç»“æœï¼Œä½¿ç”¨é›¶é¢„æµ‹")

                batch_predictions.append(aggregated_prediction)

            if not batch_predictions:
                if log_this_batch:
                    print(f"    âš ï¸ Batch {batch_idx}: æ‰€æœ‰æ‚£è€…éƒ½è¢«è·³è¿‡ï¼Œæ²¡æœ‰æœ‰æ•ˆé¢„æµ‹")
                batch_skip_count += 1
                continue

            if len(batch_predictions) != len(spot_graphs_list):
                if log_this_batch:
                    print(f"    âš ï¸ Batch {batch_idx}: {len(spot_graphs_list)}ä¸ªæ‚£è€…ä¸­åªæœ‰{len(batch_predictions)}ä¸ªæœ‰æ•ˆ")

            predictions = torch.cat(batch_predictions, dim=0)
            if log_this_batch:
                print(f"  Batch {batch_idx} åˆå¹¶é¢„æµ‹ï¼šå½¢çŠ¶={predictions.shape}")

            if predictions.shape[0] != expressions.shape[0]:
                if log_this_batch:
                    print(f"    âš ï¸ é¢„æµ‹å’ŒçœŸå®å€¼æ•°é‡ä¸åŒ¹é…: {predictions.shape[0]} vs {expressions.shape[0]}")
                expressions = expressions[:predictions.shape[0]]

            with autocast('cuda'):
                pred_sum = predictions.sum().item()
                if pred_sum <= 1e-10 or not torch.isfinite(predictions).all():
                    if log_this_batch:
                        print(f"    âŒ è­¦å‘Šï¼šé¢„æµ‹å¼‚å¸¸ï¼Œè·³è¿‡è¿™ä¸ªbatch")
                    batch_skip_count += 1
                    continue

                epsilon = 1e-8
                sum_pred = predictions.sum(dim=1, keepdim=True) + epsilon
                normalized_pred = predictions / sum_pred
                result = torch.clamp(normalized_pred * 1000000.0, min=0.0, max=1e6)

                if torch.isnan(result).any() or torch.isinf(result).any():
                    if log_this_batch:
                        print(f"    âŒ è­¦å‘Šï¼šå½’ä¸€åŒ–ç»“æœåŒ…å«NaNæˆ–Infï¼è·³è¿‡")
                    batch_skip_count += 1
                    continue

                loss = criterion(result, expressions)
                if log_this_batch:
                    print(f"  è®¡ç®—æŸå¤±ï¼š{loss.item():.6f}")
                if torch.isnan(loss) or torch.isinf(loss):
                    if log_this_batch:
                        print(f"    âŒ è­¦å‘Šï¼šæŸå¤±ä¸ºNaNæˆ–Infï¼Œè·³è¿‡è¿™ä¸ªbatch")
                    batch_skip_count += 1
                    continue

                backward_start_time = time.time() if enable_profiling else None
                if log_this_batch:
                    print(f"  å¼€å§‹åå‘ä¼ æ’­...")
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                scaler.step(optimizer)
                scaler.update()
                if log_this_batch:
                    print(f"  åå‘ä¼ æ’­å®Œæˆ")
                if enable_profiling:
                    backward_time = time.time() - backward_start_time if backward_start_time else 0.0
                    forward_time = (backward_start_time - forward_start_time) if forward_start_time else 0.0
                    total_batch_time = time.time() - batch_start_time if batch_start_time else 0.0
                    if log_this_batch:
                        print(f"  æ€§èƒ½ç»Ÿè®¡: æ€»æ—¶é—´={total_batch_time:.3f}s, æ•°æ®åŠ è½½={data_loading_time:.3f}s, å‰å‘={forward_time:.3f}s, åå‘={backward_time:.3f}s")

                running_loss += loss.item()
                num_batches += 1

                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç›‘æ§å®Œæˆåå†æ¸…ç†å¤§tensor
                del predictions, result, loss
                del batch_predictions
                del expressions, spot_graphs_list
                if cleanup_interval and cleanup_interval > 0:
                    if (batch_idx + 1) % cleanup_interval == 0 and torch.cuda.is_available():
                        torch.cuda.empty_cache()

        if num_batches == 0:
            print(f"Epoch {epoch+1}: æ‰€æœ‰batchéƒ½è¢«è·³è¿‡")
            print(f"  è·³è¿‡çš„batchæ•°: {batch_skip_count}")
            print(f"  è·³è¿‡çš„æ‚£è€…æ•°: {patient_skip_count}")
            continue

        epoch_loss = running_loss / num_batches
        train_losses.append(epoch_loss)

        print(f"\nEpoch {epoch+1} è®­ç»ƒç»Ÿè®¡:")
        print(f"  æ€»batchæ•°: {batch_idx + 1}")
        print(f"  æˆåŠŸè®­ç»ƒçš„batchæ•°: {num_batches}")
        print(f"  è·³è¿‡çš„batchæ•°: {batch_skip_count}")
        print(f"  è·³è¿‡çš„æ‚£è€…æ•°: {patient_skip_count}")
        print(f"  å¹³å‡æŸå¤±: {epoch_loss:.6f}")

        model.eval()
        test_loss = 0.0
        test_batches = 0
        with torch.no_grad():
            for batch in test_loader:
                expressions = batch['expressions'].to(device, non_blocking=True)
                spot_graphs_list = batch['spot_graphs_list']

                batch_predictions = []
                for i in range(len(spot_graphs_list)):
                    spot_graphs = spot_graphs_list[i]
                    all_cell_features = batch['all_cell_features_list'][i]
                    all_cell_positions = batch['all_cell_positions_list'][i]
                    has_graphs = batch['has_graphs_list'][i]

                    if all_cell_features.shape[0] == 0:
                        continue

                    all_cell_features = all_cell_features.to(device, non_blocking=True)
                    all_cell_positions = all_cell_positions.to(device, non_blocking=True)

                    if has_graphs and len(spot_graphs) > 0:
                        for graph in spot_graphs:
                            if hasattr(graph, 'x') and graph.x is not None:
                                graph.x = graph.x.to(device, non_blocking=True)
                            if hasattr(graph, 'edge_index') and graph.edge_index is not None:
                                graph.edge_index = graph.edge_index.to(device, non_blocking=True)

                    if has_graphs and len(spot_graphs) > 0:
                        total_cells = sum([graph.x.shape[0] for graph in spot_graphs if hasattr(graph, 'x')])
                        max_cells_threshold = 150000
                        if total_cells <= max_cells_threshold:
                            cell_predictions_list = model(spot_graphs)
                        else:
                            target_cells_per_batch = 10000
                            batch_size_adaptive = max(32, len(spot_graphs) * target_cells_per_batch // total_cells)
                            all_cell_predictions_list = []
                            for batch_start in range(0, len(spot_graphs), batch_size_adaptive):
                                batch_end = min(batch_start + batch_size_adaptive, len(spot_graphs))
                                batch_graphs = spot_graphs[batch_start:batch_end]
                                current_predictions = model(batch_graphs)
                                all_cell_predictions_list.extend(current_predictions)
                                torch.cuda.empty_cache()
                                del current_predictions
                            cell_predictions_list = all_cell_predictions_list
                    else:
                        cell_predictions = model.forward_raw_features(all_cell_features, all_cell_positions)
                        cell_predictions_list = [cell_predictions]

                    if cell_predictions_list:
                        all_cell_predictions = torch.cat([pred for pred in cell_predictions_list if pred.shape[0] > 0], dim=0)
                        if all_cell_predictions.shape[0] > 0:
                            aggregated_prediction = all_cell_predictions.sum(dim=0, keepdim=True)
                        else:
                            aggregated_prediction = torch.zeros(1, expressions.shape[1], device=device)
                    else:
                        aggregated_prediction = torch.zeros(1, expressions.shape[1], device=device)

                    batch_predictions.append(aggregated_prediction)

                if batch_predictions:
                    predictions = torch.cat(batch_predictions, dim=0)
                    sum_pred = predictions.sum(dim=1, keepdim=True).clamp(min=1e-8)
                    normalized_pred = predictions / sum_pred
                    result = normalized_pred * 1000000.0
                    loss = criterion(result, expressions)
                    if torch.isfinite(loss):
                        test_loss += loss.item()
                        test_batches += 1

                del predictions, result, loss
                del batch_predictions
                del expressions, spot_graphs_list
                torch.cuda.empty_cache()

        test_loss = test_loss / max(test_batches, 1)
        test_losses.append(test_loss)

        if scheduler is not None:
            scheduler.step()

        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train Loss: {epoch_loss:.6f}, Test Loss: {test_loss:.6f}")

        if cleanup_interval and cleanup_interval > 0:
            torch.cuda.empty_cache()
            gc.collect()

        if test_loss < best_test_loss - min_delta:
            best_test_loss = test_loss
            best_epoch = epoch + 1
            early_stopping_counter = 0
            torch.save(model.state_dict(), "best_BRCA_lora_model_transfer.pt")
            print(f"  *** ä¿å­˜æœ€ä½³æ¨¡å‹ ***")
        else:
            early_stopping_counter += 1
            if early_stopping_counter >= patience:
                print(f"æ—©åœè§¦å‘ï¼æœ€ä½³æµ‹è¯•æŸå¤±: {best_test_loss:.6f} (Epoch {best_epoch})")
                break

        if epoch_loss < best_loss:
            best_loss = epoch_loss

    print(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³æµ‹è¯•æŸå¤±: {best_test_loss:.6f}")

    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='blue')
    plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Optimized Bulk Static Training Loss (372 Genes, Multi-Graph Batch)')
    plt.legend()
    plt.grid(True)
    plt.savefig('bulk_BRCA_lora_loss_Transfer.png')
    plt.close()

    return train_losses, test_losses
